# KNN Scorer

## Overview

The **KNN Scorer** (K-Nearest Neighbors Scorer) is an embedding-based evaluation method that measures the **local density** and **uniqueness** of data points in the embedding space. This approach quantifies data diversity by computing the average distance to each sample's k-nearest neighbors. Originally proposed by Google Research as a high-fidelity data selection strategy, KNN scoring helps identify samples that are either centrally located (redundant) or peripherally positioned (unique/outlier) within the dataset's semantic space.

The core intuition is that samples with **larger K-nearest neighbor distances** are more unique or isolated in the embedding space, potentially representing diverse or valuable examples, while samples with **smaller distances** are surrounded by similar data and may be redundant.

## Metric Definition:

* **Definition:** 

  For a given data point \( x_i \) with embedding \( e_i \), the KNN distance is calculated as:

  \[
  \text{KNN\_Distance}(x_i) = \frac{1}{k} \sum_{j=1}^{k} d(e_i, e_{n_j})
  \]

  where \( e_{n_j} \) represents the embedding of the j-th nearest neighbor (excluding \( x_i \) itself), and \( d(\cdot, \cdot) \) is a distance metric (e.g., Euclidean, cosine, or Manhattan distance).

* **Explanation:** This metric quantifies how isolated or central a data point is within its local neighborhood:
  
  * A **higher KNN distance** indicates that the sample is **far from its neighbors**, suggesting it is **unique, diverse, or potentially an outlier** in the dataset.
  * A **lower KNN distance** indicates that the sample is **close to many similar samples**, suggesting it is **redundant or representative of a dense cluster**.

## YAML Configuration

```yaml
name: KNNScorer
embedding_path: /path/to/embeddings.npy
k: 5
distance_metric: euclidean
max_workers: 8
```

### Configuration Parameters

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `name` | string | `"KNNScorer"` | Identifier for the scorer |
| `embedding_path` | string | **required** | Path to the pre-computed embeddings file in `.npy` format. This file should be a 2D NumPy array with shape `(num_samples, embedding_dim)`, where each row corresponds to the embedding of a data sample in the dataset. **The order of embeddings must match the order of samples in the dataset.** |
| `k` | integer | `5` | Number of nearest neighbors to consider for distance calculation. If `k` is greater than or equal to the dataset size, it will be automatically adjusted to `num_samples - 1` |
| `distance_metric` | string | `"euclidean"` | Distance metric for computing neighbor distances. Supported values: `"euclidean"` (Euclidean/L2 distance), `"cosine"` (cosine distance = 1 - cosine similarity), `"manhattan"` (Manhattan/L1 distance) |
| `max_workers` | integer | CPU cores | Number of parallel worker processes for scoring. Higher values speed up processing but require more memory |



## Underlying Model

The KNN Scorer **does not require a specific language model**. Instead, it operates on **pre-computed embeddings** that can be generated by any embedding model suitable for your data domain, such as:

- **Text embeddings**: Sentence-BERT, OpenAI embeddings, Instructor embeddings, etc.
- **Vision embeddings**: CLIP, ResNet, Vision Transformers, etc.
- **Multimodal embeddings**: CLIP, BLIP, etc.

The choice of embedding model depends on your specific use case and data modality. The scorer is model-agnostic and works with any fixed-dimensional embedding representation.

## Scoring Process

The KNN Scorer follows these steps to evaluate each data sample:

1. **Load Embeddings**: Load the pre-computed embedding matrix from the specified `.npy` file. The embeddings should be a 2D array with shape `(num_samples, embedding_dim)`.

2. **Build KNN Index**: Construct a K-Nearest Neighbors model using the specified distance metric (e.g., Euclidean, cosine, or Manhattan). The KNN index is built on all embeddings to enable efficient neighbor searches.

3. **Find K-Nearest Neighbors**: For each data point \( x_i \):
   - Query the KNN model to find the \( k+1 \) nearest neighbors (including the point itself)
   - Exclude the first neighbor (which is the point itself) to obtain the \( k \) nearest neighbors

4. **Calculate Average Distance**: Compute the mean distance to the \( k \) nearest neighbors:
   
   \[
   \text{score}_i = \frac{1}{k} \sum_{j=1}^{k} d(e_i, e_{n_j})
   \]

5. **Parallel Processing**: The scorer uses multiprocessing with configurable worker processes to efficiently handle large datasets.

## Output Format

For each input sample, the scorer returns:

```json
{
  "id": 1,
  "score": 0.524
}
```

- `id`: The unique identifier of the data sample, extracted from the `"id"` field in the input dataset
- `score`: The average K-nearest neighbor distance for this sample. Higher scores indicate greater uniqueness or isolation in the embedding space, while lower scores indicate redundancy or centrality within a dense cluster

**Interpretation:**

- **High scores** (e.g., > 0.8): Sample is semantically distant from neighbors → potentially unique, diverse, or outlier
- **Low scores** (e.g., < 0.3): Sample is semantically close to neighbors → potentially redundant or representative of common patterns

The specific threshold values depend on the embedding model, distance metric, and dataset characteristics.

## Citation

```bibtex
@misc{google2025highfidelity,
  title        = {Achieving 10000x Training Data Reduction with High-Fidelity Labels},
  author       = {{Google Research}},
  howpublished = {\url{https://research.google/blog/achieving-10000x-training-data-reduction-with-high-fidelity-labels/}},
  note         = {Accessed: 2025-02-xx}
}
```

